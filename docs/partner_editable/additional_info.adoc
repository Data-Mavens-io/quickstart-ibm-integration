// Add steps as necessary for accessing the software, post-configuration, and testing. Don’t include full usage instructions for your software, but add links to your product documentation for that information.
//Should any sections not be applicable, remove them

== Test the deployment
// If steps are required to test the deployment, add them here. If not, remove the heading

When the AWS CloudFormation template has successfully created the stack, all server nodes are running with the software installed in your AWS account. In the following steps, connect to the {partner-product-short-name} user interface (UI) to verify the deployment, and then use the web client to explore {partner-product-short-name} features.

. To access the {partner-product-short-name} UI, go to the *CP4IWebClientURL* output of the root stack, as shown in <<cfn_outputs>>.
. A new tab opens in your browser. If you configured the hostname resolution for the cluster DNS name in the URL, you see either the {partner-product-short-name} UI login page or a warning from your browser that the HTTPS connection is not safe. By default, the public key infrastructure (PKI) certificate that is created for the {partner-product-short-name} cluster is self-signed, which causes the unsafe connection warning from your browser.
. Log in to the {partner-product-short-name} UI by using the default user admin and the password you supplied when creating the stack. If you did not supply a password, you can retrieve it from the *PlatformNavigatorPassword* secret stored in AWS Secrets Manager.
. After you log in, the welcome page is displayed as shown in <<testStep1>>.
+
:xrefstyle: short
[#testStep1]
.Welcome page for {partner-product-short-name} UI
[link=images/image6.png]
image::../images/image6.png[image_placeholder,width=648,height=439]
+
See https://www.ibm.com/demos/search/?product=Cloud+Pak+for+Integration&page=1&products=Cloud+Pak+for+Integration[IBM Demos^] for {partner-product-short-name} videos and other resources to help you understand the platform features and capabilities.


== Post-deployment steps

=== Manage your cluster using the OpenShift Console

To access the {partner-product-short-name} UI, go to the *OpenshiftURLValue* shown in <<cfn_outputs>>.

The default OpenShift administrative user is *kubeadmin.* The password is stored in AWS Secrets Manager, and the name of the secret is displayed in <<cfn_outputs>> as the value of *OpenshiftPassword*.

To retrieve the password, navigate to the AWS Systems Manager console, choose the secret displayed as the value of *OpenshiftPassword*, and then choose *Retrieve secret value* as shown in <<testStep2>>. Use the retrieved value for the OpenShift console administrative password.

:xrefstyle: short
[#testStep2]
.Retrieve secret value for OpenShift console password
[link=images/image8.png]
image::../images/image8.png[image_placeholder,width=648,height=439]

=== Access the control plane through the boot node

The recommended method of SSH access to the OpenShift cluster instances via the bastion host is by using SSH agent forwarding, as described in the following Bash instructions:

. Run the command `ssh-add -K <your-key.pem>` to store the key in your keychain. On Linux, you might need to omit the `-K` flag.
. Retrieve the host name of the boot node from the Amazon EC2 console as shown in <<testStep3>>.
+
:xrefstyle: short
[#testStep3]
.Hostname of the boot node
[link=images/image13.png]
image::../images/image13.png[image_placeholder,width=648,height=439]
+
. To log in to the bastion host, run `ssh -A ec2-user@<bootnode-host-name>``.
. Run `sudo` to become root:
+
```
$ sudo -s
```
. Run `oc login` to authenticate with OpenShift and `oc get pods`, and verify that services are in a running state:
+
```
$ oc login
$ oc get pods
```

=== Scale up your cluster by adding compute nodes

. Run `oc nodes` to get the current list of nodes.
. Run `oc get machineset -n openshift-machine-api` to get the machine sets for each Availability Zone.
+
:xrefstyle: short
[#testStep4]
.Getting the machine sets
[link=images/image14.png]
image::../images/image14.png[image_placeholder,width=648,height=439]
+
. Select the machine set to scale up from the list that is returned in the previous command.
. Edit the selected machine set and update the replica count:
+
```
oc edit machineset cp4i-pn-nk9dr-worker-eu-west-1a -n openshift-machine-api
```
+
:xrefstyle: short
[#testStep5]
.Edit machine set
[link=images/image15.png]
image::../images/image15.png[image_placeholder,width=648,height=439]
+
An AWS instance is created, and the *Desired* and *Current* counts are updated to the replica value. After a few minutes, after the node joins the cluster, the *Ready* and *Available* counts are also updated to the replica value.

NOTE: If you choose to scale down your cluster or reduce the number of compute nodes, the cluster might become unstable because pods must be rescheduled. Scaling down the worker nodes is not a recommended option.
The cluster auto scaler can overrule the scaling activity to maintain the required threshold.

=== {partner-product-short-name} services

To browse available services that you can deploy using the {partner-product-short-name} Platform Navigator, see https://www.ibm.com/support/knowledgecenter/SSGT7J_20.3/install/deploying.html[Deploying Component Products^] in the IBM Knowledge Center.

:xrefstyle: short
[#testStep6]
.Capabilities catalog page in {partner-product-short-name}
[link=images/image16.png]
image::../images/image16.png[image_placeholder,width=648,height=439]

:xrefstyle: short
[#testStep7]
.Runtimes catalog page in {partner-product-short-name}
[link=images/image17.png]
image::../images/image17.png[image_placeholder,width=648,height=439]

As part of the Quick Start installation, the Platform Navigator is installed by default, giving you the ability to choose which of the capabilities and runtimes you wish to create after the deployment has completed.

==== System requirements for each of the capabilities and services

[cols=",,",options="header",]
|===
|Service Name |CPU cores( vCPUs) |Memory
|*Asset Repository* |0.5 |640MG
|*Operations Dashboard* |7 |13GB
|*API Lifecycle and Management* |12 |48GB
|*Messaging (queue manager)* |1 |1GB
|*Event Streaming* |8.2 |8.2GB
|*Application Integration Dashboard* |1 |4GB
|*Application Integration Designer* |1 |5.75GB
|*Gateway* |4 |4GB
|*High Speed Transfers* |4 |4GB
|===


==== Installation capabilities

===== Install from the Platform Navigator

To learn more about installation capabilities using the OpenShift Platform Navigator, see https://www.ibm.com/support/knowledgecenter/SSGT7J_20.3/install/deploying.html[Deploying Component Products^] in the IBM Knowledge Center.

===== Install from the AWS boot node

. Log in to your AWS boot node server.
. Navigate to scripts directory.
+
```
cd /ibm/cp4i-deployment/capabilities-runtimes-scripts
```
. Run the command for the desired capability. See the following examples:
.. Operations Dashboard
+
```
./release-tracing.sh -n $\{namespace} -r $\{release_name} -f $\{file_storage} -b $\{block_storage} -p

# -p is optional flag, adding it installs the capability in production mode
```
+
Example:
+
```
./release-tracing.sh -n integration -r operations-dashboard -f ocs-storagecluster-cephfs -b gp2 -p
```
+
.. API Connect
+
```
./release-apic.sh -n $\{namespace} -r $\{release_name} -p

# -p is optional flag, adding it installs the capability in production mode
```
+
Example:
+
```
./release-apic.sh -n integration -r api-connect -p
```
+
.. App Connect Dashboard
+
```
./release-ace-dashboard.sh -n $\{namespace} -r $\{release_name} -s $\{storageClass} -p

# -p is optional flag, adding it installs the capability in production mode
```
+
Example:
+
```
./release-ace-dashboard.sh -n integration -r app-connect-dashboard -s ocs-storagecluster-cephfs -p
```
.. App Connect Designer
+
```
./release-ace-designer.sh -n $\{namespace} -r $\{release_name} -s $\{storageClass} -p

# -p is optional flag, adding it installs the capability in production mode
```
Example:
+
```
./release-ace-designer.sh -n integration -r app-connect-designer -s ocs-storagecluster-cephfs -p
```
.. Asset Repository
+
```
./release-ar.sh -n $\{namespace} -r $\{release_name} -a $\{assetDataVolumeClass} -c $\{couchVolumeClass}

# -p is optional flag, adding it installs the capability in production mode
```
+
Example:
+
```
./release-ar.sh --n integration -r assets-repo -a ocs-storagecluster-cephfs -c ocs-storagecluster-cephfs -p
```

. The capability now displays as *Pending* in the *Status* column, as shown in <<postDeployStep1>>. 
+
:xrefstyle: short
[#postDeployStep1]
.List capabilities
[link=images/image25.png]
image::../images/image25.png[image_placeholder,width=648,height=439]

. To trace installation progress, go to the OpenShift UI and select *Events* in the left navigation menu, as shown in <<postDeployStep2>>.
+
:xrefstyle: short
[#postDeployStep2]
.View OpenShift events
[link=images/image26.png]
image::../images/image26.png[image_placeholder,width=648,height=439]

. In the Platform Navigator, when the capability displays as *Ready* in the *Status* column, the capability is fully installed, as shown in <<postDeployStep3>>. 
+
:xrefstyle: short
[#postDeployStep3]
.Verify capability status
[link=images/image27.png]
image::../images/image27.png[image_placeholder,width=648,height=439]


For information about other services that are available, see https://www.ibm.com/support/knowledgecenter/SSGT7J_20.3/install/deploying.html[Deploying Component Products^] in the IBM Knowledge Center.

==== Changing Platform Navigator credentials

. Login to the boot node server.
. Navigate to scripts directory.
+
```
cd /ibm/cp4i-deployment/capabilities-runtimes-scripts
```
.  Run the `change-cs-credentials.sh` script with the desired credentials:
+
```
./change-cs-credentials.sh -u $\{username} -p $\{password}
```
The user name has a default value of `admin`, so if you want to keep the user name and only change the password, you can skip the `-u` flag.
+
```
./change-cs-credentials.sh -p $\{password}
```
Note that changing credentials requires you to use the `cloudctl` command line interface (CLI). The script automatically downloads the `cloudctl` CLI from the deployed common services on your OpenShift cluster and uses it, so you don't have to pre-install it. 
